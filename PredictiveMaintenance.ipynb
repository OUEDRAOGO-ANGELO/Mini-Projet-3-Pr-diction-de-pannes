{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124494 entries, 0 to 124493\n",
      "Data columns (total 12 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   date     124494 non-null  datetime64[ns]\n",
      " 1   device   124494 non-null  object        \n",
      " 2   failure  124494 non-null  int64         \n",
      " 3   metric1  124494 non-null  int64         \n",
      " 4   metric2  124494 non-null  int64         \n",
      " 5   metric3  124494 non-null  int64         \n",
      " 6   metric4  124494 non-null  int64         \n",
      " 7   metric5  124494 non-null  int64         \n",
      " 8   metric6  124494 non-null  int64         \n",
      " 9   metric7  124494 non-null  int64         \n",
      " 10  metric8  124494 non-null  int64         \n",
      " 11  metric9  124494 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(10), object(1)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"predictive_maintenance_dataset.csv\")\n",
    "data[\"date\"] = data[\"date\"].apply(lambda col: pd.to_datetime(col, format=\"%m/%d/%Y\"))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_max'] = data.groupby(\"device\")['date'].transform('max')\n",
    "data['RUL'] = data['date_max'] - data['date']\n",
    "data[\"RUL\"] = data[\"RUL\"].dt.days\n",
    "data = data.drop([\"date_max\", \"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Failed = data \n",
    "Failures = Failed.groupby('device').agg({'failure' : 'max'})\n",
    "Failures = Failures.rename(columns=({'failure': 'relevant'}))\n",
    "Failed = Failed.merge(Failures, how='left', on='device')\n",
    "Failed = Failed[Failed['relevant']==1]\n",
    "Failed = Failed.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Failed = Failed.drop(['failure','relevant'], axis=1)\n",
    "Failed = Failed.drop('device', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network on RUL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = Failed.drop('RUL', axis=1)\n",
    "y = Failed['RUL']\n",
    "X = StandardScaler().fit_transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy pour MLPclassifier: 0.001866368047779022\n",
      "ecart aboslu moyen: 30.8006718924972\n",
      "ecart absolu inférieur à 10: 1062\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLPClassifier(hidden_layer_sizes=(400, 300, 300, 300, 300, 300, 400), max_iter=1000, random_state=42, activation='relu', solver='adam')\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "predictions_mlp = model_mlp.predict(X_test)\n",
    "\n",
    "# Évaluation de l'efficacité du modèle\n",
    "accuracy_mlp = accuracy_score(y_test, predictions_mlp)\n",
    "print(f'Test accuracy pour MLPclassifier: {accuracy_mlp}')\n",
    "\n",
    "ecart = predictions_mlp - y_test\n",
    "ecart_abs = np.abs(ecart)\n",
    "print(f'ecart aboslu moyen: {np.mean(ecart_abs)}')\n",
    "print(f'ecart absolu inférieur à 10: {(ecart_abs < 10).sum() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ecart_abs < 15).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [(Failed['RUL'] < 7), (Failed['RUL'] < 15), (Failed['RUL'] < 30)]\n",
    "values = ['Critique', 'Important', 'Modere']\n",
    "Failed['Criticité'] = np.select(conditions, values, default='Secondaire')\n",
    "Failed_class = Failed.drop('RUL', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Failed_class['Criticité']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le pourcentage de bien classés est de : 88.05524449421426 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "weights = {'Critique' : 5, 'Important':2, 'Modere':1, 'Secondaire':2}\n",
    "model_rf = RandomForestClassifier(n_estimators=10, class_weight=weights)\n",
    "model_rf.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(f\"Le pourcentage de bien classés est de : {accuracy_score(y_test, model_rf.predict(X_test))*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Critique_predit</th>\n",
       "      <th>Important_predit</th>\n",
       "      <th>Modere_predit</th>\n",
       "      <th>Secondaire_predit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Critique_données</th>\n",
       "      <td>120</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Important_données</th>\n",
       "      <td>28</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modere_données</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>222</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Secondaire_données</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Critique_predit  Important_predit  Modere_predit  \\\n",
       "Critique_données                120                19              3   \n",
       "Important_données                28                94             35   \n",
       "Modere_données                    2                43            222   \n",
       "Secondaire_données                8                13             53   \n",
       "\n",
       "                    Secondaire_predit  \n",
       "Critique_données                   19  \n",
       "Important_données                  23  \n",
       "Modere_données                     74  \n",
       "Secondaire_données               1923  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, model_rf.predict(X_test)),\n",
    "             index = [\"Critique_données\", \"Important_données\", \"Modere_données\", \"Secondaire_données\"],\n",
    "             columns = [\"Critique_predit\", \"Important_predit\", \"Modere_predit\", \"Secondaire_predit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy pour MLPclassifier: 0.7353490108249346\n"
     ]
    }
   ],
   "source": [
    "MLPClassifier(hidden_layer_sizes=(400, 300, 300, 300, 300, 300, 400), max_iter=1000, random_state=42, activation='relu', solver='adam')\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "predictions_mlp = model_mlp.predict(X_test)\n",
    "\n",
    "# Évaluation de l'efficacité du modèle\n",
    "accuracy_mlp = accuracy_score(y_test, predictions_mlp)\n",
    "print(f'Test accuracy pour MLPclassifier: {accuracy_mlp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
